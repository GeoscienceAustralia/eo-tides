{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e .. --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import odc.stac\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "\n",
    "from pyTMD.compute import tide_elevations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "GAUGE_X = 122.2183\n",
    "GAUGE_Y = -18.0008\n",
    "ENSEMBLE_MODELS = [\"EOT20\", \"HAMTIDE11\"]  # simplified for tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_satellite_ds():\n",
    "    \"\"\"\n",
    "    Load a sample timeseries of Landsat 8 data using odc-stac\n",
    "    \"\"\"\n",
    "    # Connect to stac catalogue\n",
    "    catalog = pystac_client.Client.open(\"https://explorer.dea.ga.gov.au/stac\")\n",
    "\n",
    "    # Set cloud defaults\n",
    "    odc.stac.configure_rio(\n",
    "        cloud_defaults=True,\n",
    "        aws={\"aws_unsigned\": True},\n",
    "    )\n",
    "\n",
    "    # Build a query with the parameters above\n",
    "    buffer = 0.08\n",
    "    # buffer = 0.5\n",
    "    bbox = [GAUGE_X - buffer, GAUGE_Y - buffer, GAUGE_X + buffer, GAUGE_Y + buffer]\n",
    "    query = catalog.search(\n",
    "        bbox=bbox,\n",
    "        collections=[\"ga_ls8c_ard_3\"],\n",
    "        datetime=\"2020-01/2020-02\",\n",
    "    )\n",
    "\n",
    "    # Search the STAC catalog for all items matching the query\n",
    "    ds = odc.stac.load(\n",
    "        list(query.items()),\n",
    "        bands=[\"nbart_red\"],\n",
    "        crs=\"epsg:3577\",\n",
    "        resolution=30,\n",
    "        groupby=\"solar_day\",\n",
    "        bbox=bbox,\n",
    "        fail_on_error=False,\n",
    "        chunks={},\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "satellite_ds = load_satellite_ds()\n",
    "\n",
    "def load_measured_tides_ds():\n",
    "    \"\"\"\n",
    "    Load measured sea level data from the Broome ABSLMP tidal station:\n",
    "    http://www.bom.gov.au/oceanography/projects/abslmp/data/data.shtml\n",
    "    \"\"\"\n",
    "    # Metadata for Broome ABSLMP tidal station:\n",
    "    # http://www.bom.gov.au/oceanography/projects/abslmp/data/data.shtml\n",
    "    ahd_offset = -5.322\n",
    "\n",
    "    # Load measured tides from ABSLMP tide gauge data\n",
    "    measured_tides_df = pd.read_csv(\n",
    "        \"../tests/data/IDO71013_2020.csv\",\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "        na_values=-9999,\n",
    "    )[[\"Sea Level\"]]\n",
    "\n",
    "    # Update index and column names\n",
    "    measured_tides_df.index.name = \"time\"\n",
    "    measured_tides_df.columns = [\"tide_height\"]\n",
    "\n",
    "    # Apply station AHD offset\n",
    "    measured_tides_df += ahd_offset\n",
    "\n",
    "    # Return as xarray dataset\n",
    "    return measured_tides_df.to_xarray()\n",
    "\n",
    "satellite_ds = load_satellite_ds()\n",
    "measured_tides_ds = load_measured_tides_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing pyTMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eo_tides import model_tides\n",
    "\n",
    "x, y, crs, method, model = GAUGE_X, GAUGE_Y, \"EPSG:4326\", \"spline\", \"EOT20\"\n",
    "x, y, crs, method, model = GAUGE_X, GAUGE_Y, \"EPSG:4326\", \"bilinear\", \"EOT20\"\n",
    "x, y, crs, method, model = -1034913, -1961916, \"EPSG:3577\", \"bilinear\", \"EOT20\"\n",
    "\n",
    "\n",
    "# Run EOT20 tidal model for locations and timesteps in tide gauge data\n",
    "modelled_tides_df = model_tides(\n",
    "    x=[x],\n",
    "    y=[y],\n",
    "    time=measured_tides_ds.time,\n",
    "    crs=crs,\n",
    "    method=method,\n",
    "    directory=\"../tests/data/tide_models\",\n",
    ")\n",
    "\n",
    "# Run equivalent pyTMD code to verify same results\n",
    "pytmd_tides = tide_elevations(\n",
    "        x=x, \n",
    "        y=y, \n",
    "        delta_time=measured_tides_ds.time,\n",
    "        DIRECTORY=\"../tests/data/tide_models\",\n",
    "        MODEL=\"EOT20\",\n",
    "        EPSG=int(crs[-4:]),\n",
    "        TIME=\"datetime\",\n",
    "        EXTRAPOLATE=True,\n",
    "        CUTOFF=np.inf,\n",
    "        METHOD=method,\n",
    "        # CORRECTIONS: str | None = None,\n",
    "        # INFER_MINOR: bool = True,\n",
    "        # MINOR_CONSTITUENTS: list | None = None,\n",
    "        # APPLY_FLEXURE: bool = False,\n",
    "        # FILL_VALUE: float = np.nan\n",
    "        )\n",
    "\n",
    "np.allclose(modelled_tides_df.tide_height.values, pytmd_tides.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error for out of bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelling tides using EOT20, GOT5.5 in parallel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The EOT20 tide model constituent files do not cover the requested analysis extent.\nThis can occur if you are using clipped model files to improve run times.\nConsider using model files that cover your entire analysis area, or set `crop=False`\nto reduce the extent of tide model constituent files that is loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/env/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/env/lib/python3.10/concurrent/futures/process.py\", line 205, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/env/lib/python3.10/concurrent/futures/process.py\", line 205, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/home/jovyan/Robbi/eo-tides/eo_tides/model.py\", line 299, in _model_tides\n    raise Exception(textwrap.dedent(error_msg).strip()) from None\nException: The EOT20 tide model constituent files do not cover the requested analysis extent.\nThis can occur if you are using clipped model files to improve run times.\nConsider using model files that cover your entire analysis area, or set `crop=False`\nto reduce the extent of tide model constituent files that is loaded.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run EOT20 tidal model for locations and timesteps in tide gauge data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m modelled_tides_df \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_tides\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEOT20\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGOT5.5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeasured_tides_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../tests/data/tide_models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Robbi/eo-tides/eo_tides/model.py:807\u001b[0m, in \u001b[0;36mmodel_tides\u001b[0;34m(x, y, time, model, directory, crs, crop, method, extrapolate, cutoff, mode, parallel, parallel_splits, output_units, output_format, ensemble_models, **ensemble_kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Apply func in parallel, iterating through each input param\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 807\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_iters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_iters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BrokenProcessPool:\n\u001b[1;32m    814\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParallelised tide modelling failed, likely to to an out-of-memory error. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry reducing the size of your analysis, or set `parallel=False`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m     )\n",
      "File \u001b[0;32m/env/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/env/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/env/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/env/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/env/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/env/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: The EOT20 tide model constituent files do not cover the requested analysis extent.\nThis can occur if you are using clipped model files to improve run times.\nConsider using model files that cover your entire analysis area, or set `crop=False`\nto reduce the extent of tide model constituent files that is loaded."
     ]
    }
   ],
   "source": [
    "from eo_tides import model_tides\n",
    "\n",
    "x, y = 180, -50\n",
    "\n",
    "\n",
    "# Run EOT20 tidal model for locations and timesteps in tide gauge data\n",
    "modelled_tides_df = model_tides(\n",
    "    x=[x],\n",
    "    y=[y],\n",
    "    model=[\"EOT20\", \"GOT5.5\"],\n",
    "    time=measured_tides_ds.time,\n",
    "    directory=\"../tests/data/tide_models\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eo_tides import list_models\n",
    "list_models(directory=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling ebb and flow tidal phases\n",
    "The `tag_tides` function also allows us to determine whether each satellite observation was taken while the tide was rising/incoming (flow tide) or falling/outgoing (ebb tide) by setting `ebb_flow=True`. This is achieved by comparing tide heights 15 minutes before and after the observed satellite observation.\n",
    "\n",
    "Ebb and flow data can provide valuable contextual information for interpreting satellite imagery, particularly in tidal flat or mangrove forest environments where water may remain in the landscape for considerable time after the tidal peak.\n",
    "\n",
    "Once you run the cell below, our data will now also contain a new `ebb_flow` variable under **Data variables**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "ds = dc.load(product=\"ga_s2ls_intertidal_cyear_3\", limit=1, measurements=\"elevation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GeoBox((3200, 3200), Affine(10.0, 0.0, 1248000.0,\n",
       "        0.0, -10.0, -1184000.0), CRS('PROJCS[\"GDA94 / Australian Albers\",GEOGCS[\"GDA94\",DATUM[\"Geocentric_Datum_of_Australia_1994\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6283\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4283\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",132],PARAMETER[\"standard_parallel_1\",-18],PARAMETER[\"standard_parallel_2\",-36],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3577\"]]')),\n",
       " array(['2022-02-01T00:00:00.000000000'], dtype='datetime64[ns]'))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from odc.geo.geobox import GeoBox\n",
    "import xarray as xr\n",
    "import textwrap\n",
    "import numpy as np\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def _standardise_time(\n",
    "    time: np.ndarray | pd.DatetimeIndex | pd.Timestamp | None,\n",
    ") -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    Accept a datetime64 ndarray, pandas.DatetimeIndex\n",
    "    or pandas.Timestamp, and return a datetime64 ndarray.\n",
    "    \"\"\"\n",
    "    # Return time as-is if none\n",
    "    if time is None:\n",
    "        return time\n",
    "\n",
    "    # Convert to a 1D datetime64 array\n",
    "    time = np.atleast_1d(time).astype(\"datetime64[ns]\")\n",
    "\n",
    "    return time\n",
    "\n",
    "\n",
    "def _standardise_inputs(\n",
    "    ds: xr.DataArray | xr.Dataset | GeoBox,\n",
    "    time: np.ndarray | pd.DatetimeIndex | pd.Timestamp | None,\n",
    ") -> (GeoBox, np.ndarray):\n",
    "    \"\"\"\n",
    "    Takes an xarray or GeoBox input and an optional custom times,\n",
    "    and returns a standardised GeoBox and  \n",
    "    \"\"\"\n",
    "\n",
    "    # If `ds` is an xarray object, extract its GeoBox and time\n",
    "    if isinstance(ds, (xr.DataArray, xr.Dataset)):\n",
    "\n",
    "        # Try to extract GeoBox\n",
    "        try:\n",
    "            gbox = ds.odc.geobox\n",
    "        except AttributeError:\n",
    "            error_msg = \"\"\"\n",
    "            Cannot extract a valid GeoBox for `ds`. This is required for\n",
    "            extracting details about `ds`'s CRS and spatial location.\n",
    "            \n",
    "            Import `odc.geo.xr` then run `ds = ds.odc.assign_crs(crs=...)`\n",
    "            to prepare your data before passing it to this function.\n",
    "            \"\"\"\n",
    "            raise Exception(textwrap.dedent(error_msg).strip())\n",
    "\n",
    "        # Use custom time by default if provided; otherwise try and extract from `ds`\n",
    "        if time is not None:\n",
    "            time = _standardise_time(time)\n",
    "        elif \"time\" in ds.coords:\n",
    "            time = ds.coords[\"time\"].values\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"`ds` does not have a time dimension, and no custom times were provided via `time`.\"\n",
    "            )\n",
    "\n",
    "    # If `ds` is a GeoBox, use it directly; raise an error if no time was provided\n",
    "    elif isinstance(ds, GeoBox):\n",
    "        gbox = ds\n",
    "        if time is not None:\n",
    "            time = _standardise_time(time)\n",
    "        else:\n",
    "            raise ValueError(\"If `ds` is a GeoBox, `time` must be provided.\")\n",
    "\n",
    "    # Raise error if no valid inputs were provided\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            \"`ds` must be an xarray.DataArray, xarray.Dataset, or odc.geo.geobox.GeoBox.\"\n",
    "        )\n",
    "\n",
    "    return gbox, time\n",
    "\n",
    "\n",
    "time = pd.date_range(\"2021\", \"2022\").values\n",
    "time = pd.date_range(\"2021\", \"2022\")\n",
    "time = pd.Timestamp(\"2022-02-01\")\n",
    "# time = satellite_ds.time\n",
    "# time = [\"a\", \"b\"]\n",
    "\n",
    "\n",
    "gbox, time = _standardise_inputs(ds=ds.drop_dims(\"time\").odc.geobox, time=time)\n",
    "gbox, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(609,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satellite_ds.chunks[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2021-01-01T00:00:00.000000000', '2021-01-02T00:00:00.000000000',\n",
       "       '2021-01-03T00:00:00.000000000', '2021-01-04T00:00:00.000000000',\n",
       "       '2021-01-05T00:00:00.000000000', '2021-01-06T00:00:00.000000000',\n",
       "       '2021-01-07T00:00:00.000000000', '2021-01-08T00:00:00.000000000',\n",
       "       '2021-01-09T00:00:00.000000000', '2021-01-10T00:00:00.000000000',\n",
       "       '2021-01-11T00:00:00.000000000', '2021-01-12T00:00:00.000000000',\n",
       "       '2021-01-13T00:00:00.000000000', '2021-01-14T00:00:00.000000000',\n",
       "       '2021-01-15T00:00:00.000000000', '2021-01-16T00:00:00.000000000',\n",
       "       '2021-01-17T00:00:00.000000000', '2021-01-18T00:00:00.000000000',\n",
       "       '2021-01-19T00:00:00.000000000', '2021-01-20T00:00:00.000000000',\n",
       "       '2021-01-21T00:00:00.000000000', '2021-01-22T00:00:00.000000000',\n",
       "       '2021-01-23T00:00:00.000000000', '2021-01-24T00:00:00.000000000',\n",
       "       '2021-01-25T00:00:00.000000000', '2021-01-26T00:00:00.000000000',\n",
       "       '2021-01-27T00:00:00.000000000', '2021-01-28T00:00:00.000000000',\n",
       "       '2021-01-29T00:00:00.000000000', '2021-01-30T00:00:00.000000000',\n",
       "       '2021-01-31T00:00:00.000000000', '2021-02-01T00:00:00.000000000',\n",
       "       '2021-02-02T00:00:00.000000000', '2021-02-03T00:00:00.000000000',\n",
       "       '2021-02-04T00:00:00.000000000', '2021-02-05T00:00:00.000000000',\n",
       "       '2021-02-06T00:00:00.000000000', '2021-02-07T00:00:00.000000000',\n",
       "       '2021-02-08T00:00:00.000000000', '2021-02-09T00:00:00.000000000',\n",
       "       '2021-02-10T00:00:00.000000000', '2021-02-11T00:00:00.000000000',\n",
       "       '2021-02-12T00:00:00.000000000', '2021-02-13T00:00:00.000000000',\n",
       "       '2021-02-14T00:00:00.000000000', '2021-02-15T00:00:00.000000000',\n",
       "       '2021-02-16T00:00:00.000000000', '2021-02-17T00:00:00.000000000',\n",
       "       '2021-02-18T00:00:00.000000000', '2021-02-19T00:00:00.000000000',\n",
       "       '2021-02-20T00:00:00.000000000', '2021-02-21T00:00:00.000000000',\n",
       "       '2021-02-22T00:00:00.000000000', '2021-02-23T00:00:00.000000000',\n",
       "       '2021-02-24T00:00:00.000000000', '2021-02-25T00:00:00.000000000',\n",
       "       '2021-02-26T00:00:00.000000000', '2021-02-27T00:00:00.000000000',\n",
       "       '2021-02-28T00:00:00.000000000', '2021-03-01T00:00:00.000000000',\n",
       "       '2021-03-02T00:00:00.000000000', '2021-03-03T00:00:00.000000000',\n",
       "       '2021-03-04T00:00:00.000000000', '2021-03-05T00:00:00.000000000',\n",
       "       '2021-03-06T00:00:00.000000000', '2021-03-07T00:00:00.000000000',\n",
       "       '2021-03-08T00:00:00.000000000', '2021-03-09T00:00:00.000000000',\n",
       "       '2021-03-10T00:00:00.000000000', '2021-03-11T00:00:00.000000000',\n",
       "       '2021-03-12T00:00:00.000000000', '2021-03-13T00:00:00.000000000',\n",
       "       '2021-03-14T00:00:00.000000000', '2021-03-15T00:00:00.000000000',\n",
       "       '2021-03-16T00:00:00.000000000', '2021-03-17T00:00:00.000000000',\n",
       "       '2021-03-18T00:00:00.000000000', '2021-03-19T00:00:00.000000000',\n",
       "       '2021-03-20T00:00:00.000000000', '2021-03-21T00:00:00.000000000',\n",
       "       '2021-03-22T00:00:00.000000000', '2021-03-23T00:00:00.000000000',\n",
       "       '2021-03-24T00:00:00.000000000', '2021-03-25T00:00:00.000000000',\n",
       "       '2021-03-26T00:00:00.000000000', '2021-03-27T00:00:00.000000000',\n",
       "       '2021-03-28T00:00:00.000000000', '2021-03-29T00:00:00.000000000',\n",
       "       '2021-03-30T00:00:00.000000000', '2021-03-31T00:00:00.000000000',\n",
       "       '2021-04-01T00:00:00.000000000', '2021-04-02T00:00:00.000000000',\n",
       "       '2021-04-03T00:00:00.000000000', '2021-04-04T00:00:00.000000000',\n",
       "       '2021-04-05T00:00:00.000000000', '2021-04-06T00:00:00.000000000',\n",
       "       '2021-04-07T00:00:00.000000000', '2021-04-08T00:00:00.000000000',\n",
       "       '2021-04-09T00:00:00.000000000', '2021-04-10T00:00:00.000000000',\n",
       "       '2021-04-11T00:00:00.000000000', '2021-04-12T00:00:00.000000000',\n",
       "       '2021-04-13T00:00:00.000000000', '2021-04-14T00:00:00.000000000',\n",
       "       '2021-04-15T00:00:00.000000000', '2021-04-16T00:00:00.000000000',\n",
       "       '2021-04-17T00:00:00.000000000', '2021-04-18T00:00:00.000000000',\n",
       "       '2021-04-19T00:00:00.000000000', '2021-04-20T00:00:00.000000000',\n",
       "       '2021-04-21T00:00:00.000000000', '2021-04-22T00:00:00.000000000',\n",
       "       '2021-04-23T00:00:00.000000000', '2021-04-24T00:00:00.000000000',\n",
       "       '2021-04-25T00:00:00.000000000', '2021-04-26T00:00:00.000000000',\n",
       "       '2021-04-27T00:00:00.000000000', '2021-04-28T00:00:00.000000000',\n",
       "       '2021-04-29T00:00:00.000000000', '2021-04-30T00:00:00.000000000',\n",
       "       '2021-05-01T00:00:00.000000000', '2021-05-02T00:00:00.000000000',\n",
       "       '2021-05-03T00:00:00.000000000', '2021-05-04T00:00:00.000000000',\n",
       "       '2021-05-05T00:00:00.000000000', '2021-05-06T00:00:00.000000000',\n",
       "       '2021-05-07T00:00:00.000000000', '2021-05-08T00:00:00.000000000',\n",
       "       '2021-05-09T00:00:00.000000000', '2021-05-10T00:00:00.000000000',\n",
       "       '2021-05-11T00:00:00.000000000', '2021-05-12T00:00:00.000000000',\n",
       "       '2021-05-13T00:00:00.000000000', '2021-05-14T00:00:00.000000000',\n",
       "       '2021-05-15T00:00:00.000000000', '2021-05-16T00:00:00.000000000',\n",
       "       '2021-05-17T00:00:00.000000000', '2021-05-18T00:00:00.000000000',\n",
       "       '2021-05-19T00:00:00.000000000', '2021-05-20T00:00:00.000000000',\n",
       "       '2021-05-21T00:00:00.000000000', '2021-05-22T00:00:00.000000000',\n",
       "       '2021-05-23T00:00:00.000000000', '2021-05-24T00:00:00.000000000',\n",
       "       '2021-05-25T00:00:00.000000000', '2021-05-26T00:00:00.000000000',\n",
       "       '2021-05-27T00:00:00.000000000', '2021-05-28T00:00:00.000000000',\n",
       "       '2021-05-29T00:00:00.000000000', '2021-05-30T00:00:00.000000000',\n",
       "       '2021-05-31T00:00:00.000000000', '2021-06-01T00:00:00.000000000',\n",
       "       '2021-06-02T00:00:00.000000000', '2021-06-03T00:00:00.000000000',\n",
       "       '2021-06-04T00:00:00.000000000', '2021-06-05T00:00:00.000000000',\n",
       "       '2021-06-06T00:00:00.000000000', '2021-06-07T00:00:00.000000000',\n",
       "       '2021-06-08T00:00:00.000000000', '2021-06-09T00:00:00.000000000',\n",
       "       '2021-06-10T00:00:00.000000000', '2021-06-11T00:00:00.000000000',\n",
       "       '2021-06-12T00:00:00.000000000', '2021-06-13T00:00:00.000000000',\n",
       "       '2021-06-14T00:00:00.000000000', '2021-06-15T00:00:00.000000000',\n",
       "       '2021-06-16T00:00:00.000000000', '2021-06-17T00:00:00.000000000',\n",
       "       '2021-06-18T00:00:00.000000000', '2021-06-19T00:00:00.000000000',\n",
       "       '2021-06-20T00:00:00.000000000', '2021-06-21T00:00:00.000000000',\n",
       "       '2021-06-22T00:00:00.000000000', '2021-06-23T00:00:00.000000000',\n",
       "       '2021-06-24T00:00:00.000000000', '2021-06-25T00:00:00.000000000',\n",
       "       '2021-06-26T00:00:00.000000000', '2021-06-27T00:00:00.000000000',\n",
       "       '2021-06-28T00:00:00.000000000', '2021-06-29T00:00:00.000000000',\n",
       "       '2021-06-30T00:00:00.000000000', '2021-07-01T00:00:00.000000000',\n",
       "       '2021-07-02T00:00:00.000000000', '2021-07-03T00:00:00.000000000',\n",
       "       '2021-07-04T00:00:00.000000000', '2021-07-05T00:00:00.000000000',\n",
       "       '2021-07-06T00:00:00.000000000', '2021-07-07T00:00:00.000000000',\n",
       "       '2021-07-08T00:00:00.000000000', '2021-07-09T00:00:00.000000000',\n",
       "       '2021-07-10T00:00:00.000000000', '2021-07-11T00:00:00.000000000',\n",
       "       '2021-07-12T00:00:00.000000000', '2021-07-13T00:00:00.000000000',\n",
       "       '2021-07-14T00:00:00.000000000', '2021-07-15T00:00:00.000000000',\n",
       "       '2021-07-16T00:00:00.000000000', '2021-07-17T00:00:00.000000000',\n",
       "       '2021-07-18T00:00:00.000000000', '2021-07-19T00:00:00.000000000',\n",
       "       '2021-07-20T00:00:00.000000000', '2021-07-21T00:00:00.000000000',\n",
       "       '2021-07-22T00:00:00.000000000', '2021-07-23T00:00:00.000000000',\n",
       "       '2021-07-24T00:00:00.000000000', '2021-07-25T00:00:00.000000000',\n",
       "       '2021-07-26T00:00:00.000000000', '2021-07-27T00:00:00.000000000',\n",
       "       '2021-07-28T00:00:00.000000000', '2021-07-29T00:00:00.000000000',\n",
       "       '2021-07-30T00:00:00.000000000', '2021-07-31T00:00:00.000000000',\n",
       "       '2021-08-01T00:00:00.000000000', '2021-08-02T00:00:00.000000000',\n",
       "       '2021-08-03T00:00:00.000000000', '2021-08-04T00:00:00.000000000',\n",
       "       '2021-08-05T00:00:00.000000000', '2021-08-06T00:00:00.000000000',\n",
       "       '2021-08-07T00:00:00.000000000', '2021-08-08T00:00:00.000000000',\n",
       "       '2021-08-09T00:00:00.000000000', '2021-08-10T00:00:00.000000000',\n",
       "       '2021-08-11T00:00:00.000000000', '2021-08-12T00:00:00.000000000',\n",
       "       '2021-08-13T00:00:00.000000000', '2021-08-14T00:00:00.000000000',\n",
       "       '2021-08-15T00:00:00.000000000', '2021-08-16T00:00:00.000000000',\n",
       "       '2021-08-17T00:00:00.000000000', '2021-08-18T00:00:00.000000000',\n",
       "       '2021-08-19T00:00:00.000000000', '2021-08-20T00:00:00.000000000',\n",
       "       '2021-08-21T00:00:00.000000000', '2021-08-22T00:00:00.000000000',\n",
       "       '2021-08-23T00:00:00.000000000', '2021-08-24T00:00:00.000000000',\n",
       "       '2021-08-25T00:00:00.000000000', '2021-08-26T00:00:00.000000000',\n",
       "       '2021-08-27T00:00:00.000000000', '2021-08-28T00:00:00.000000000',\n",
       "       '2021-08-29T00:00:00.000000000', '2021-08-30T00:00:00.000000000',\n",
       "       '2021-08-31T00:00:00.000000000', '2021-09-01T00:00:00.000000000',\n",
       "       '2021-09-02T00:00:00.000000000', '2021-09-03T00:00:00.000000000',\n",
       "       '2021-09-04T00:00:00.000000000', '2021-09-05T00:00:00.000000000',\n",
       "       '2021-09-06T00:00:00.000000000', '2021-09-07T00:00:00.000000000',\n",
       "       '2021-09-08T00:00:00.000000000', '2021-09-09T00:00:00.000000000',\n",
       "       '2021-09-10T00:00:00.000000000', '2021-09-11T00:00:00.000000000',\n",
       "       '2021-09-12T00:00:00.000000000', '2021-09-13T00:00:00.000000000',\n",
       "       '2021-09-14T00:00:00.000000000', '2021-09-15T00:00:00.000000000',\n",
       "       '2021-09-16T00:00:00.000000000', '2021-09-17T00:00:00.000000000',\n",
       "       '2021-09-18T00:00:00.000000000', '2021-09-19T00:00:00.000000000',\n",
       "       '2021-09-20T00:00:00.000000000', '2021-09-21T00:00:00.000000000',\n",
       "       '2021-09-22T00:00:00.000000000', '2021-09-23T00:00:00.000000000',\n",
       "       '2021-09-24T00:00:00.000000000', '2021-09-25T00:00:00.000000000',\n",
       "       '2021-09-26T00:00:00.000000000', '2021-09-27T00:00:00.000000000',\n",
       "       '2021-09-28T00:00:00.000000000', '2021-09-29T00:00:00.000000000',\n",
       "       '2021-09-30T00:00:00.000000000', '2021-10-01T00:00:00.000000000',\n",
       "       '2021-10-02T00:00:00.000000000', '2021-10-03T00:00:00.000000000',\n",
       "       '2021-10-04T00:00:00.000000000', '2021-10-05T00:00:00.000000000',\n",
       "       '2021-10-06T00:00:00.000000000', '2021-10-07T00:00:00.000000000',\n",
       "       '2021-10-08T00:00:00.000000000', '2021-10-09T00:00:00.000000000',\n",
       "       '2021-10-10T00:00:00.000000000', '2021-10-11T00:00:00.000000000',\n",
       "       '2021-10-12T00:00:00.000000000', '2021-10-13T00:00:00.000000000',\n",
       "       '2021-10-14T00:00:00.000000000', '2021-10-15T00:00:00.000000000',\n",
       "       '2021-10-16T00:00:00.000000000', '2021-10-17T00:00:00.000000000',\n",
       "       '2021-10-18T00:00:00.000000000', '2021-10-19T00:00:00.000000000',\n",
       "       '2021-10-20T00:00:00.000000000', '2021-10-21T00:00:00.000000000',\n",
       "       '2021-10-22T00:00:00.000000000', '2021-10-23T00:00:00.000000000',\n",
       "       '2021-10-24T00:00:00.000000000', '2021-10-25T00:00:00.000000000',\n",
       "       '2021-10-26T00:00:00.000000000', '2021-10-27T00:00:00.000000000',\n",
       "       '2021-10-28T00:00:00.000000000', '2021-10-29T00:00:00.000000000',\n",
       "       '2021-10-30T00:00:00.000000000', '2021-10-31T00:00:00.000000000',\n",
       "       '2021-11-01T00:00:00.000000000', '2021-11-02T00:00:00.000000000',\n",
       "       '2021-11-03T00:00:00.000000000', '2021-11-04T00:00:00.000000000',\n",
       "       '2021-11-05T00:00:00.000000000', '2021-11-06T00:00:00.000000000',\n",
       "       '2021-11-07T00:00:00.000000000', '2021-11-08T00:00:00.000000000',\n",
       "       '2021-11-09T00:00:00.000000000', '2021-11-10T00:00:00.000000000',\n",
       "       '2021-11-11T00:00:00.000000000', '2021-11-12T00:00:00.000000000',\n",
       "       '2021-11-13T00:00:00.000000000', '2021-11-14T00:00:00.000000000',\n",
       "       '2021-11-15T00:00:00.000000000', '2021-11-16T00:00:00.000000000',\n",
       "       '2021-11-17T00:00:00.000000000', '2021-11-18T00:00:00.000000000',\n",
       "       '2021-11-19T00:00:00.000000000', '2021-11-20T00:00:00.000000000',\n",
       "       '2021-11-21T00:00:00.000000000', '2021-11-22T00:00:00.000000000',\n",
       "       '2021-11-23T00:00:00.000000000', '2021-11-24T00:00:00.000000000',\n",
       "       '2021-11-25T00:00:00.000000000', '2021-11-26T00:00:00.000000000',\n",
       "       '2021-11-27T00:00:00.000000000', '2021-11-28T00:00:00.000000000',\n",
       "       '2021-11-29T00:00:00.000000000', '2021-11-30T00:00:00.000000000',\n",
       "       '2021-12-01T00:00:00.000000000', '2021-12-02T00:00:00.000000000',\n",
       "       '2021-12-03T00:00:00.000000000', '2021-12-04T00:00:00.000000000',\n",
       "       '2021-12-05T00:00:00.000000000', '2021-12-06T00:00:00.000000000',\n",
       "       '2021-12-07T00:00:00.000000000', '2021-12-08T00:00:00.000000000',\n",
       "       '2021-12-09T00:00:00.000000000', '2021-12-10T00:00:00.000000000',\n",
       "       '2021-12-11T00:00:00.000000000', '2021-12-12T00:00:00.000000000',\n",
       "       '2021-12-13T00:00:00.000000000', '2021-12-14T00:00:00.000000000',\n",
       "       '2021-12-15T00:00:00.000000000', '2021-12-16T00:00:00.000000000',\n",
       "       '2021-12-17T00:00:00.000000000', '2021-12-18T00:00:00.000000000',\n",
       "       '2021-12-19T00:00:00.000000000', '2021-12-20T00:00:00.000000000',\n",
       "       '2021-12-21T00:00:00.000000000', '2021-12-22T00:00:00.000000000',\n",
       "       '2021-12-23T00:00:00.000000000', '2021-12-24T00:00:00.000000000',\n",
       "       '2021-12-25T00:00:00.000000000', '2021-12-26T00:00:00.000000000',\n",
       "       '2021-12-27T00:00:00.000000000', '2021-12-28T00:00:00.000000000',\n",
       "       '2021-12-29T00:00:00.000000000', '2021-12-30T00:00:00.000000000',\n",
       "       '2021-12-31T00:00:00.000000000', '2022-01-01T00:00:00.000000000'],\n",
       "      dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time = pd.date_range(\"2021\", \"2022\").values\n",
    "# time = pd.date_range(\"2021\", \"2022\")\n",
    "# time = pd.Timestamp(\"2022-02-01\")\n",
    "time = satellite_ds.time\n",
    "\n",
    "\n",
    "def _standardise_time(\n",
    "    time: np.ndarray | pd.DatetimeIndex | pd.Timestamp | None,\n",
    ") -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    Accept a datetime64 ndarray, pandas.DatetimeIndex\n",
    "    or pandas.Timestamp, and return a datetime64 ndarray.\n",
    "    \"\"\"\n",
    "    # Return time as-is if none\n",
    "    if time is None:\n",
    "        return time\n",
    "\n",
    "    # Convert to a 1D datetime64 array\n",
    "    time = np.atleast_1d(time).astype(\"datetime64[ns]\")\n",
    "\n",
    "    return time\n",
    "\n",
    "\n",
    "time = pd.date_range(\"2021\", \"2022\").values\n",
    "# time = pd.date_range(\"2021\", \"2022\")\n",
    "# time = pd.Timestamp(\"2022-02-01\")\n",
    "# time = satellite_ds.time\n",
    "# time = [pd.Timestamp(\"2022-02-01\"), pd.Timestamp(\"2022-02-01\")]\n",
    "# time = None\n",
    "_standardise_time(time=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.atleast_1d(time).astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2022-02-01T00:00:00.000000000', '2022-02-01T00:00:00.000000000'],\n",
       "      dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.odc.assign_crs(\"EPSG:3577\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = satellite_ds.nbart_red.drop_attrs(deep=True).drop_vars(\"spatial_ref\").odc.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test  #odc.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model tide heights\n",
    "ds = tag_tides(\n",
    "    ds, \n",
    "    ebb_flow=True,     \n",
    "    directory=\"../../tests/data/tide_models\",\n",
    ")\n",
    "\n",
    "# Print output data\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have data giving us the both the tide height and tidal phase (\"ebb\" or \"flow\") for every satellite image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[[\"time\", \"tide_height\", \"ebb_flow\"]].drop_vars(\"spatial_ref\").to_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could for example use this data to filter our observations to keep ebbing phase observations only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ebb = ds.where(ds.ebb_flow == \"Ebb\", drop=True)\n",
    "print(ds_ebb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import odc.stac\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "\n",
    "# Connect to STAC catalog\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "\n",
    "# Set cloud access defaults\n",
    "odc.stac.configure_rio(\n",
    "    cloud_defaults=True,\n",
    "    aws={\"aws_unsigned\": True},\n",
    ")\n",
    "\n",
    "# Build a query and search the STAC catalog for all matching items\n",
    "bbox = [122.160, -18.05, 122.260, -17.95]\n",
    "query = catalog.search(\n",
    "    bbox=bbox,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    datetime=\"2021/2023\",\n",
    ")\n",
    "\n",
    "# Load data into xarray format\n",
    "ds_s2 = odc.stac.load(\n",
    "    items=list(query.items()),\n",
    "    bands=[\"red\"],\n",
    "    crs=\"utm\",\n",
    "    resolution=30,\n",
    "    groupby=\"solar_day\",\n",
    "    bbox=bbox,\n",
    "    fail_on_error=False,\n",
    "    chunks={},\n",
    ")\n",
    "\n",
    "print(ds_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(stats_ds.data_vars.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eo_tides.stats import pixel_stats\n",
    "\n",
    "models = [\"EOT20\"]\n",
    "resample = True\n",
    "\n",
    "stats_ds = pixel_stats(\n",
    "    ds=satellite_ds,\n",
    "    model=models,\n",
    "    resample=resample,\n",
    "    directory=\"../tests/data/tide_models\",\n",
    ")\n",
    "\n",
    "# Verify dims are correct\n",
    "assert stats_ds.odc.spatial_dims == satellite_ds.odc.spatial_dims\n",
    "\n",
    "# Verify vars are as expected\n",
    "expected_vars = ['hat',  'hot',  'lat',  'lot',  'otr',  'tr',  'spread',  'offset_low',  'offset_high']\n",
    "assert set(expected_vars) == set(stats_ds.data_vars)\n",
    "\n",
    "# Verify tide models are correct\n",
    "assert all(stats_ds[\"tide_model\"].values == models)\n",
    "if len(models) > 1:\n",
    "    assert \"tide_model\" in stats_ds.dims\n",
    "\n",
    "# If resample, assert that statistics have the same shape and dims\n",
    "# as `satellite_ds`\n",
    "if resample:\n",
    "    assert satellite_ds.odc.geobox.shape == stats_ds.odc.geobox.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify values are roughly expected\n",
    "assert np.allclose(stats_ds.offset_high.mean().item, 0.30, atol=0.02)\n",
    "assert np.allclose(stats_ds.offset_low.mean().item, 0.27, atol=0.02)\n",
    "assert np.allclose(stats_ds.spread.mean().item, 0.43, atol=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ds.offset_high.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ds.spread.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ds[\"tide_model\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ds[\"tide_model\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(['hat',  'hot',  'lat',  'lot',  'otr',  'tr',  'spread',  'offset_low',  'offset_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stats_ds.data_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eo_tides import pixel_tides\n",
    "\n",
    "pixel_tides(\n",
    "    ds=satellite_ds,\n",
    "    model=[\"EOT20\", \"GOT5.5\"],\n",
    "    directory=\"../tests/data/tide_models\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ds.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_ds.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
